{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "import sklearn.metrics\n",
    "import sklearn\n",
    "import lazypredict\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tnrange\n",
    "import itertools\n",
    "from sklearn.impute import KNNImputer\n",
    "import tqdm\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "import optuna\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from fitter import Fitter, get_common_distributions, get_distributions\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\n",
    "    \"train_airbnb\": \"../DATA/train_airbnb_berlin.xls\",\n",
    "    \"berlin_crimes\": \"../DATA/extra_datasets/Berlin_crimes.csv\",\n",
    "    \"berlin_demographics\": \"../DATA/extra_datasets/Berlin_demographics.csv\"\n",
    "}\n",
    "\n",
    "def cast_district(x):\n",
    "    return x.replace(\"NeukÃ¶lln\",\"Neukölln\").replace('Tempelhof - SchÃ¶neberg','Tempelhof - Schöneberg').replace('Treptow - KÃ¶penick','Treptow - Köpenick').replace('Charlottenburg-Wilm.','Charlottenburg-Wilmersdorf').replace('Ã¶','ö').replace('Ã¤','ä').replace('Ã','ß')\n",
    "\n",
    "def get_training_data(data_files):\n",
    "    df_train = pd.read_csv(data_files['train_airbnb'])\n",
    "\n",
    "    #crimes_df = pd.read_csv(data_files['berlin_crimes'])\n",
    "    #crimes_df = crimes_df.groupby(['District','Year']).sum().groupby('District').mean().drop('Code', axis=1)\n",
    "\n",
    "    #demographics_df = pd.read_csv(data_files['berlin_demographics'], delimiter=';', encoding='latin-1')\n",
    "    #demographics_df['Code Postal'] = demographics_df.apply(lambda x: x['Bezirk']*10000+x['Ortsteil']*10+x['Geschl'], axis=1)\n",
    "    #demographics_df = demographics_df[['Code Postal', 'Bez-Name', 'Ortst-Name', 'Staatsangeh','Altersgr', 'Häufigkeit']]\n",
    "    #demographics_df = demographics_df.rename(columns = {\"Bez-Name\":\"District\",\"Häufigkeit\": \"Population\", \"Altersgr\":\"Tranche d'âge\",\"Staatsangeh\":\"Origine\"})\n",
    "    #number_of_inhabitants_df = demographics_df.groupby(['Code Postal','District','Ortst-Name']).sum()\n",
    "    #number_of_germans_df = demographics_df.groupby(['Code Postal','District','Ortst-Name','Origine']).sum().query('Origine==\"D\"').rename(columns = {\"Population\":\"Nb Allemands\"})\n",
    "    #number_of_immigrants_df = demographics_df.groupby(['Code Postal','District','Ortst-Name','Origine']).sum().query('Origine==\"A\"').rename(columns = {\"Population\":\"Nb Immigrants\"})\n",
    "    #demographics_df = number_of_inhabitants_df.merge(number_of_germans_df, how='inner', on=['Code Postal','District','Ortst-Name']).merge(number_of_immigrants_df, how='inner', on=['Code Postal','District','Ortst-Name'])\n",
    "    #demographics_df['Proportion Allemands'] = demographics_df.apply(lambda x: x['Nb Allemands']/x['Population'], axis=1)\n",
    "    #demographics_df['Proportion Immigrants'] = demographics_df.apply(lambda x: x['Nb Immigrants']/x['Population'], axis=1)\n",
    "    #demographics_df = demographics_df[['Population','Proportion Allemands','Proportion Immigrants']]\n",
    "\n",
    "    #extra_data_df = demographics_df.reset_index().merge(crimes_df, how='left', on='District')\n",
    "    #for column in extra_data_df.columns:\n",
    "        #if column in ['Robbery','Street_robbery', 'Injury', 'Agg_assault', 'Threat', 'Theft', 'Car','From_car', 'Bike', 'Burglary', 'Fire', 'Arson', 'Damage', 'Graffiti','Drugs', 'Local']:\n",
    "            #extra_data_df[column] = extra_data_df.apply(lambda x: x[column]/x['Population'], axis=1)\n",
    "    \n",
    "    #df_train['neighbourhood'] = df_train['neighbourhood'].apply(lambda x:cast_district(x))\n",
    "    #df_train['Neighborhood Group'] = df_train['Neighborhood Group'].apply(lambda x:cast_district(x))\n",
    "    #df_train = df_train.merge(extra_data_df, how='left', left_on='neighbourhood', right_on='Ortst-Name')\n",
    "\n",
    "    return df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = get_training_data(data_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subplot_analysis():\n",
    "    plt.subplot(221)\n",
    "    df_train.boxplot(column='Price')\n",
    "\n",
    "    plt.subplot(222)\n",
    "    df_train.boxplot(column='Square Feet')\n",
    "\n",
    "    plt.subplot(223)\n",
    "    df_train.boxplot(column='Value Rating')\n",
    "\n",
    "    plt.subplot(224)\n",
    "    df_train.boxplot(column='Accuracy Rating')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_of_nans(df: pd.DataFrame) -> list:\n",
    "    nan_percentage = pd.DataFrame(columns=['Feature Name', 'Percentage of NaNs'])\n",
    "    for idx, feature in enumerate(df.columns):\n",
    "        notnans = df_train[feature].notna().value_counts()\n",
    "        percentage = 0\n",
    "        if False in notnans:\n",
    "            percentage = int(10000*notnans[False]/len(df_train))/100\n",
    "\n",
    "        nan_percentage.loc[idx] = [feature, percentage]\n",
    "    return nan_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_percentage_analysis():\n",
    "    nan_percentage = percentage_of_nans(df_train)\n",
    "    df = nan_percentage[nan_percentage['Percentage of NaNs'] >= 18]\n",
    "    df.sort_values(by=['Percentage of NaNs'], ascending=False).reset_index().drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_raw(df: pd.DataFrame):\n",
    "    df.columns = df.columns. str.lower().str.replace(' ','_')         \n",
    "\n",
    "    # Converting to datetime\n",
    "    df.host_since = pd.to_datetime(df.host_since)               \n",
    "\n",
    "    cols_to_drop = ['listing_name', 'host_id', 'host_name', 'city', 'country_code', 'country', \n",
    "                    'first_review', 'last_review', 'neighbourhood', 'business_travel_ready',  'postal_code']\n",
    "    df = df.drop(cols_to_drop, axis=1)\n",
    "    df.set_index('listing_id', inplace=True)\n",
    "\n",
    "    df.drop(['square_feet'], axis=1, inplace=True)\n",
    "\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].replace('*', np.nan)\n",
    "\n",
    "    df = df.dropna(subset=['price'], how='all')\n",
    "    df = df.dropna(subset=['overall_rating', 'accuracy_rating','cleanliness_rating','checkin_rating',\n",
    "                            'communication_rating','location_rating','value_rating'], how='all')\n",
    "    \n",
    "    change = ['accomodates','latitude',\n",
    "          'longitude','bathrooms','bedrooms','beds','guests_included','min_nights','reviews','overall_rating','accuracy_rating',\n",
    "          'cleanliness_rating','checkin_rating', 'communication_rating','location_rating','value_rating']\n",
    "\n",
    "    label_encoded = ['host_response_time', 'is_superhost', 'is_exact_location', 'instant_bookable']\n",
    "\n",
    "    categorical_variables = ['room_type', 'property_type','neighborhood_group'] \n",
    "    for i in change:\n",
    "        df[i]=pd.to_numeric(df[i], downcast=\"float\")\n",
    "\n",
    "    #Replacing the null values\n",
    "\n",
    "    df['host_response_rate'] = df['host_response_rate'].map(lambda x: str(x), na_action='ignore').map(lambda x : int(x[:-1]), na_action='ignore') \n",
    "\n",
    "    for cat_var in categorical_variables:\n",
    "        dummie = pd.get_dummies(df[cat_var])\n",
    "        df = pd.concat([df, dummie], axis=1)\n",
    "\n",
    "    df.drop(categorical_variables, axis=1, inplace=True)\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    for i in label_encoded:\n",
    "        df[i] = le.fit_transform(df[i].astype(str))\n",
    "    \n",
    "    X = df.drop('price', axis = 1)\n",
    "\n",
    "    # Output/Dependent variable\n",
    "\n",
    "    y = df['price']\n",
    "    return X, y\n",
    "\n",
    "def median_imputer(df: pd.DataFrame):\n",
    "    change = ['accomodates','latitude',\n",
    "        'longitude','bathrooms','bedrooms','beds','guests_included','min_nights','reviews','overall_rating','accuracy_rating',\n",
    "        'cleanliness_rating','checkin_rating', 'communication_rating','location_rating','value_rating' ]\n",
    "    \n",
    "    df = df.copy()\n",
    "\n",
    "    if 'host_since' in df.columns:\n",
    "        # Calculating the number of days    \n",
    "        df['host_days_active'] = (datetime(2019, 7, 21) - df.host_since).astype('timedelta64[D]').copy()\n",
    "\n",
    "        # Replacing null values with the median\n",
    "        df['host_days_active'] = df.host_days_active.fillna(df.host_days_active.median()).copy()\n",
    "\n",
    "        df['host_days_active'] = (datetime(2019, 7, 21) - df.host_since).astype('timedelta64[D]').copy()\n",
    "        df.drop('host_since', axis=1, inplace=True)\n",
    "\n",
    "    for col in change:\n",
    "        if col not in df.columns:continue\n",
    "        df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "    df = df.fillna(0).copy()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_clf_scaler_cv(clf, X, y, clf_params, pipeline_params={}, n_features=50):\n",
    "    X = X.copy()\n",
    "    y = y.copy()\n",
    "    X = X.reset_index(drop=True)\n",
    "    y = y.reset_index(drop=True)\n",
    "    kf = KFold(n_splits=5)\n",
    "    \n",
    "    rmse_list = []\n",
    "    r2_list = []\n",
    "    X = median_imputer(X)\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "        y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "\n",
    "        pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA(n_components=n_features)),\n",
    "        ('svr', clf(**clf_params))], **pipeline_params)\n",
    "        \n",
    "        pipe.fit(X_train,y_train)\n",
    "        y_pred = pipe.predict(X_test)\n",
    "        rmse = mean_squared_error(y_pred, y_test)**(1/2)\n",
    "        rmse_list.append(rmse)\n",
    "        r2_list.append(pipe.score(X_test, y_test))\n",
    "    return np.array(rmse_list).mean(), np.array(r2_list).mean()\n",
    "\n",
    "def fit_clf_scaler(clf, X, y, clf_params, pipeline_params={}, n_features=50):\n",
    "    X = X.copy()\n",
    "    y = y.copy()\n",
    "    X = X.reset_index(drop=True)\n",
    "    y = y.reset_index(drop=True)\n",
    "    \n",
    "    pipe = Pipeline([('median_imputer', FunctionTransformer(median_imputer)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=n_features)),\n",
    "    ('svr', clf(**clf_params))], **pipeline_params)\n",
    "    \n",
    "    pipe.fit(X,y)\n",
    "    return pipe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = pre_process_raw(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_analysis():\n",
    "    plt.hist(y_train,bins=100, label='train')\n",
    "    plt.hist(y_test, bins=100,label='test')\n",
    "    plt.legend()\n",
    "    plt.grid(axis='x')\n",
    "    plt.show()\n",
    "\n",
    "    f = Fitter(y.values,\n",
    "            distributions=['gamma',\n",
    "                            'lognorm',\n",
    "                            \"beta\",\n",
    "                            \"burr\",\n",
    "                            \"norm\"])\n",
    "    f.fit()\n",
    "    f.summary()\n",
    "\n",
    "    f_train = Fitter(y_train.values, distributions=['burr'])\n",
    "    f_train.fit()\n",
    "\n",
    "    display(f_train.get_best())\n",
    "\n",
    "    f_test = Fitter(y_test.values, distributions=['burr'])\n",
    "    f_test.fit()\n",
    "    display(f_test.get_best())\n",
    "\n",
    "    r_train = burr.rvs(**(f_train.get_best()['burr']),size=1000)\n",
    "    r_test = burr.rvs(**(f_test.get_best()['burr']),size=1000)\n",
    "    r_test_base = burr.rvs(**(f_test.get_best()['burr']),size=1000)\n",
    "    print('KL divergence between train and test: ', sum(rel_entr(r_train, r_test)))\n",
    "    print('KL divergence reference: ', sum(rel_entr(r_test, r_test_base)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pca_analysis():\n",
    "    pipe = Pipeline([('median_imputer', FunctionTransformer(median_imputer)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA())])\n",
    "\n",
    "    pipe.fit(X_train)\n",
    "    explained_variance = pipe['pca'].explained_variance_ratio_\n",
    "    acc_sum = np.cumsum(explained_variance) \n",
    "\n",
    "    plt.title('Cumulative explained variance')\n",
    "    plt.plot(acc_sum)\n",
    "    plt.grid()\n",
    "    plt.ylabel('Explained variance')\n",
    "    plt.xlabel('Number of features')\n",
    "    plt.show()\n",
    "\n",
    "    print('Explained variance with 50 features: ', acc_sum[45])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward selection + Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importance(clf, params):\n",
    "\tr2 = [] \n",
    "\trmse = []\n",
    "\tfeatures = []\n",
    "\tn_features = []\n",
    "\n",
    "\tfor k in range(1,len(X.columns) + 1):\n",
    "\t\tr2_tmp = []\n",
    "\t\trmse_tmp = []\n",
    "\t\tfor new_feat in X_train.columns:\n",
    "\t\t\tif new_feat in features: continue\n",
    "\t\t\tnew_features = features.copy()\n",
    "\t\t\tnew_features.append(new_feat)\n",
    "\t\t\tpipe = fit_clf_scaler(clf, X_train[new_features],y_train, params, {}, n_features=len(new_features))   \n",
    "\t\t\tr_squared = pipe.score(X_test[new_features],y_test)\n",
    "\t\t\ty_pred = pipe.predict(X_test[new_features])\n",
    "\t\t\trmse_val = mean_squared_error(y_pred, y_test)**(1/2)\n",
    "\t\t\tr2_tmp.append((r_squared, new_feat))                 \n",
    "\t\t\trmse_tmp.append(rmse_val)\n",
    "\t\tr2.append(sorted(r2_tmp, key=lambda x: x[0])[-1][0])\t\n",
    "\t\trmse.append(sorted(rmse_tmp)[-1])\n",
    "\t\tnew_feat = sorted(r2_tmp, key=lambda x: x[0])[-1][1]\n",
    "\t\tfeatures.append(new_feat)\n",
    "\t\tn_features.append(len(features))   \n",
    "\n",
    "\tdf = pd.DataFrame({'n_features': n_features,'R2': r2, 'RMSE':rmse ,'features':features})\n",
    "\n",
    "\treturn df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subset_selection_analysis():\n",
    "    df = get_feature_importance(linear_model.LinearRegression, {})\n",
    "    plt.title('Cross-validated subset selection - Linear Regression')\n",
    "    plt.xlabel('Number of features')\n",
    "    plt.ylabel('R squared')\n",
    "    plt.grid()\n",
    "    plt.plot(df['n_features'], df['R2'])\n",
    "    plt.show()\n",
    "\n",
    "    plt.title('Cross-validated subset selection - Linear Regression')\n",
    "    plt.xlabel('Number of features')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.grid()\n",
    "    plt.plot(df['n_features'], df['RMSE'])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decision_tree_analysis():\n",
    "    rmse = []\n",
    "    max_depth = []\n",
    "    for k in range(1, 30):\n",
    "        rmse_val = fit_clf_scaler_cv(DecisionTreeRegressor, X_train,y_train, {\"max_depth\": k}, {\"verbose\": 0}, n_features=25) \n",
    "        rmse.append(rmse_val)\t\n",
    "        max_depth.append(k)\n",
    "\n",
    "    df = pd.DataFrame({'RMSE': rmse,'max_depth':max_depth})\n",
    "    plt.title('Influence of the max depth - Decision Tree Regression - 5 fold CV')\n",
    "    plt.xlabel('Max depth')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.grid()\n",
    "    plt.plot(df['max_depth'], df['RMSE'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r_squared:  0.31403471430716545\n",
      "rmse:  40.01658608039282\n"
     ]
    }
   ],
   "source": [
    "clf_params = {'max_depth': 4, 'min_samples_split': 2}\n",
    "rmse, r_squared = fit_clf_scaler_cv(DecisionTreeRegressor, X_train, y_train, clf_params)\n",
    "\n",
    "print('r_squared: ', r_squared)\n",
    "print('rmse: ', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_smv():\n",
    "    def objective(trial: optuna.trial.Trial):\n",
    "\n",
    "        parameters = {\n",
    "            'kernel': trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "            'C': trial.suggest_float('C', 1e-3, 1e3),\n",
    "            'epsilon': trial.suggest_float('epsilon', 1e-3, 1)\n",
    "        }\n",
    "\n",
    "        clf = svm.SVR\n",
    "        return -fit_clf_scaler(clf, X_train, y_train, parameters)\n",
    "\n",
    "    study = optuna.create_study()\n",
    "    study.optimize(objective, n_trials=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r_squared:  0.27329516235459844\n",
      "rmse:  41.24319778781084\n"
     ]
    }
   ],
   "source": [
    "clf_params = {'kernel': 'rbf'}#, 'C': 595.6756199722245, 'epsilon': 0.8771538699650302}\n",
    "rmse, r_squared = fit_clf_scaler_cv(svm.SVR, X_train, y_train, clf_params, {\"verbose\": 0})\n",
    "\n",
    "print('r_squared: ', r_squared)\n",
    "print('rmse: ', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .... (step 1 of 4) Processing median_imputer, total=   0.0s\n",
      "[Pipeline] ............ (step 2 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 3 of 4) Processing pca, total=   0.9s\n",
      "[Pipeline] ............... (step 4 of 4) Processing svr, total=  21.4s\n",
      "r_squared:  0.33655831510958\n",
      "rmse:  39.16692711061652\n"
     ]
    }
   ],
   "source": [
    "clf_params = {'kernel': 'rbf'}#, 'C': 595.6756199722245, 'epsilon': 0.8771538699650302}\n",
    "pipe = fit_clf_scaler(svm.SVR, X_train, y_train, clf_params, {\"verbose\": 1})\n",
    "r_squared = pipe.score(X_test,y_test)\n",
    "y_pred = pipe.predict(X_test)\n",
    "rmse = mean_squared_error(y_pred, y_test)**(1/2)\n",
    "\n",
    "print('r_squared: ', r_squared)\n",
    "print('rmse: ', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_adaboost():\n",
    "    def objective(trial: optuna.trial.Trial):\n",
    "\n",
    "        parameters = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 1, 200),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-6, 1)\n",
    "        }\n",
    "\n",
    "        clf = AdaBoostRegressor\n",
    "        return -fit_clf_scaler(clf, X_train, y_train, parameters, n_features=5)\n",
    "\n",
    "    study = optuna.create_study()\n",
    "    study.optimize(objective, n_trials=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r_squared:  -0.8278628931220486\n",
      "rmse:  64.60127861678401\n"
     ]
    }
   ],
   "source": [
    "clf_params = {'n_estimators': 119, 'learning_rate': 0.9617041908836014}\n",
    "rmse, r_squared = fit_clf_scaler_cv(AdaBoostRegressor, X_train, y_train, clf_params, {\"verbose\": 0})\n",
    "\n",
    "print('r_squared: ', r_squared)\n",
    "print('rmse: ', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_knn():\n",
    "    def objective(trial: optuna.trial.Trial):\n",
    "\n",
    "        trial_suggestion = {\n",
    "            'n_neighbors': trial.suggest_int('n_neighbors', 1, 200),\n",
    "            'leaf_size': trial.suggest_int('leaf_size', 10, 50),\n",
    "        }\n",
    "\n",
    "        clf_params = {\n",
    "            'n_neighbors': trial_suggestion['n_neighbors'],\n",
    "            'leaf_size': trial_suggestion['leaf_size'],\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        imputer_params = {\n",
    "            \"n_neighbors\": 3\n",
    "        }\n",
    "\n",
    "        clf = KNeighborsRegressor\n",
    "        return fit_clf_scaler_cv(clf, X_train, y_train, clf_params, imputer_params, n_features=28)\n",
    "\n",
    "    study = optuna.create_study()\n",
    "    study.optimize(objective, n_trials=30, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r_squared:  0.38255890047340396\n",
      "rmse:  38.00313486098797\n"
     ]
    }
   ],
   "source": [
    "clf_params =  {'n_neighbors': 84, 'leaf_size': 26}\n",
    "rmse, r_squared = fit_clf_scaler_cv(KNeighborsRegressor, X_train, y_train, clf_params, {\"verbose\": 0})\n",
    "print('r_squared: ', r_squared)\n",
    "print('rmse: ', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Tree Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_gradient_boost():\n",
    "    def objective(trial: optuna.trial.Trial):\n",
    "\n",
    "        trial_suggestion = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 1, 200),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-6, 1),\n",
    "            'max_depth': trial.suggest_int('max_depth', 1, 20),\n",
    "            'n_neighbors': trial.suggest_int('n_neighbors',  2, 4),\n",
    "            'n_features': trial.suggest_int('n_features', 10, 54)\n",
    "        }\n",
    "\n",
    "        clf_params = {\n",
    "            'n_estimators': trial_suggestion['n_estimators'],\n",
    "            'learning_rate': trial_suggestion['learning_rate'],\n",
    "            'max_depth': trial_suggestion['max_depth'],\n",
    "        }\n",
    "        imputer_params = {\n",
    "            \"n_neighbors\": trial_suggestion['n_neighbors']\n",
    "        }\n",
    "\n",
    "        clf = GradientBoostingRegressor\n",
    "        return fit_clf_scaler_cv(clf, X_train, y_train, clf_params, imputer_params, n_features=trial_suggestion['n_features'])\n",
    "\n",
    "    study = optuna.create_study()\n",
    "    study.optimize(objective, n_trials=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r_squared:  0.43583816434933464\n",
      "rmse:  36.335879959122224\n"
     ]
    }
   ],
   "source": [
    "clf_params = {'n_estimators': 178, 'learning_rate': 0.15253399946567545, 'max_depth': 2} \n",
    "rmse, r_squared = fit_clf_scaler_cv(GradientBoostingRegressor, X_train, y_train, clf_params, {\"verbose\": 0})\n",
    "\n",
    "print('r_squared: ', r_squared)\n",
    "print('rmse: ', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "def optimize_mlp():\n",
    "    def objective(trial: optuna.trial.Trial):\n",
    "\n",
    "        trial_suggestions = {\n",
    "            'first_layer': trial.suggest_int('first_layer', 1, 100),\n",
    "            'second_layer': trial.suggest_int('second_layer', 1, 100),\n",
    "            'activation': trial.suggest_categorical('activation', ['relu', 'logistic', 'tanh', 'relu'])\n",
    "        }\n",
    "\n",
    "        clf = MLPRegressor \n",
    "        clf_params = {\n",
    "            'hidden_layer_sizes': (trial_suggestions['first_layer'], trial_suggestions['second_layer']),#, trial_suggestions['third_layer']),\n",
    "            'activation': trial_suggestions['activation'],\n",
    "            'max_iter': 200,\n",
    "            'learning_rate': 'adaptive'\n",
    "        }\n",
    "        rmse, r_squared = fit_clf_scaler_cv(clf, X_train, y_train, clf_params, {\"verbose\": 0})\n",
    "        return rmse\n",
    "\n",
    "    study = optuna.create_study()\n",
    "    study.optimize(objective, n_trials=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r_squared:  0.30530548635178556\n",
      "rmse:  40.30930851552766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "clf_params = {\n",
    "    \"hidden_layer_sizes\": (93, 47),\n",
    "    \"activation\": 'tanh'\n",
    "}\n",
    "rmse, r_squared = fit_clf_scaler_cv(sklearn.neural_network.MLPRegressor , X_train, y_train, clf_params, {\"verbose\": 0})\n",
    "\n",
    "print('r_squared: ', r_squared)\n",
    "print('rmse: ', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .... (step 1 of 4) Processing median_imputer, total=   0.0s\n",
      "[Pipeline] ............ (step 2 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 3 of 4) Processing pca, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing svr, total=  40.7s\n",
      "r_squared:  0.39140806252137494\n",
      "rmse:  33.35246856130338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "clf_params = {\n",
    "    \"hidden_layer_sizes\": (93, 47),\n",
    "    \"activation\": 'logistic'\n",
    "}\n",
    "pipe = fit_clf_scaler(sklearn.neural_network.MLPRegressor, X_train, y_train, clf_params, {\"verbose\": 1})\n",
    "r_squared = pipe.score(X_test,y_test)\n",
    "y_pred = pipe.predict(X_test)\n",
    "rmse = mean_squared_error(y_pred, y_test)**(1/2)\n",
    "\n",
    "print('r_squared: ', r_squared)\n",
    "print('rmse: ', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "clf_params = {\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"n_estimators\":6000,\n",
    "    \"max_depth\":4,\n",
    "    \"min_child_weight\":0,\n",
    "    \"gamma\":0.6,\n",
    "    \"subsample\":0.7,\n",
    "    \"colsample_bytree\":0.7,\n",
    "    \"objective\":'reg:squarederror',\n",
    "    \"nthread\":-1,\n",
    "    \"scale_pos_weight\":1,\n",
    "    \"seed\":27,\n",
    "    \"reg_alpha\":0.00006,\n",
    "    \"random_state\":42\n",
    "}\n",
    "rmse, r_squared = fit_clf_scaler_cv(XGBRegressor, X_train, y_train, clf_params, {\"verbose\": 0})\n",
    "\n",
    "print('r_squared: ', r_squared)\n",
    "print('rmse: ', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_random_forest():\n",
    "    def objective(trial: optuna.trial.Trial):\n",
    "\n",
    "        trial_suggestion = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 1, 200),\n",
    "            'max_depth': trial.suggest_int('max_depth', 1, 20),\n",
    "            'n_neighbors': trial.suggest_int('n_neighbors',  2, 4),\n",
    "        }\n",
    "\n",
    "        clf_params = {\n",
    "            \"n_estimators\": trial_suggestion['n_estimators'],\n",
    "            \"criterion\": \"squared_error\",\n",
    "\n",
    "        }\n",
    "        imputer_params = {\n",
    "            \"n_neighbors\": trial_suggestion['n_neighbors']\n",
    "        }\n",
    "\n",
    "        clf = GradientBoostingRegressor\n",
    "        return fit_clf_scaler_cv(clf, X_train, y_train, clf_params, imputer_params, n_features=28)\n",
    "\n",
    "    study = optuna.create_study()\n",
    "    study.optimize(objective, n_trials=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r_squared:  0.40183903267545507\n",
      "rmse:  36.441424499650196\n"
     ]
    }
   ],
   "source": [
    "clf_params = {'n_estimators': 134, 'max_depth': 5}\n",
    "rmse, r_squared = fit_clf_scaler_cv(RandomForestRegressor, X_train, y_train, clf_params, {\"verbose\": 0})\n",
    "\n",
    "print('r_squared: ', r_squared)\n",
    "print('rmse: ', rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
